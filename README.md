This is a continuation of the Pupper project at the Aarhus Business Academy/Erhvervsakademi Aarhus Create Lab, being developed by two interns.  
The goal is the creation of an autonumous robot based on the Stanford 'Pupper' robot, to form a repository of knowledge that can be used for future robotics courses and Hackathon events.

Please note that while the Stanford Pupper is programmed by Stanford to be used with a PlayStation controller, our Pupper has been reprogrammed with more autonomy in mind, to instead receive its commands through message injection.

We are currently making use of two peripheral devices, a RPLIDAR A2 LIDAR, and a HUSKYLENS camera.

The LIDAR:  
The RPLIDAR A2 is a LIDAR developed by SLAMTEC. 
With it we hope to have the Pupper walk around obstacles on its own, and, in the long run, implement SLAM, Simultaneous Location And Mapping.

The camera:  
The HUSKYLENS is an AI machine vision sensor developed by DFRobot.
We want to use it to add more complex object/people recognition to the Pupper, allowing it to identify what is in front of it.

If you are not using these devices, or want a clean start to add your own, please see the "No-Peripheral-Devices" branch.
